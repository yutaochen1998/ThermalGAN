{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ec06b-99c6-47f7-92b3-012c39f279f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Concatenate, LeakyReLU, Activation, Layer, InputSpec\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from IPython import display\n",
    "from time import perf_counter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# preserve threads for GPU\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "\n",
    "# constrain VRAM usage\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# enable mixed-precision training\n",
    "keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e911a24-5fca-4b4d-9e2b-e68a29a641fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding, **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = (input_shape[0], input_shape[1] + 2*self.padding[0], input_shape[2] + 2*self.padding[1], input_shape[3])\n",
    "        return shape\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        width_pad, height_pad = self.padding\n",
    "        return tf.pad(x, [[0, 0], [height_pad, height_pad], [width_pad, width_pad], [0, 0]], 'REFLECT')\n",
    "\n",
    "def define_discriminator(input_shape, name='Discriminator'):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    input_image = Input(shape=input_shape)\n",
    "    d = Conv2D(64, 4, strides=2, padding='same', kernel_initializer=init)(input_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(128, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(256, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(512, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(512, 4, padding='same', kernel_initializer=init, use_bias=False)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    output = Conv2D(1, 4, padding='same', kernel_initializer=init, name='Patch', dtype=tf.float32)(d)\n",
    "    model = Model(input_image, output, name=name)\n",
    "    return model\n",
    "\n",
    "def resnet_block(n_filters, input_layer):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    g = ReflectionPadding2D(padding=(1, 1))(input_layer)\n",
    "    g = Conv2D(n_filters, 3, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = ReflectionPadding2D(padding=(1, 1))(g)\n",
    "    g = Conv2D(n_filters, 3, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Concatenate()([g, input_layer])\n",
    "    return g\n",
    "\n",
    "def define_generator(input_shape, output_channel, n_resnet=9, name='Generator'):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    input_image = Input(shape=input_shape)\n",
    "    g = ReflectionPadding2D(padding=(3, 3))(input_image)\n",
    "    g = Conv2D(64, 7, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = ReflectionPadding2D(padding=(1, 1))(g)\n",
    "    g = Conv2D(128, 3, strides=2, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = ReflectionPadding2D(padding=(1, 1))(g)\n",
    "    g = Conv2D(256, 3, strides=2, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    for _ in range(n_resnet):\n",
    "        g = resnet_block(256, g)\n",
    "    g = Conv2DTranspose(128, 3, strides=2, padding='same', kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Conv2DTranspose(64, 3, strides=2, padding='same', kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = ReflectionPadding2D(padding=(3, 3))(g)\n",
    "    g = Conv2D(output_channel, 7, kernel_initializer=init)(g)\n",
    "    output_image = Activation('tanh', dtype=tf.float32)(g)\n",
    "    model = Model(input_image, output_image, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba384c18-678e-4796-b445-2d65faa3a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_train_preprocessing(sample):\n",
    "    image = tf.io.read_file(sample)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.\n",
    "    return image\n",
    "\n",
    "def data_train_augmentation(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    if random.random() < 0.5:\n",
    "        image = tf.image.rot90(image)\n",
    "    return image\n",
    "\n",
    "def data_val_preprocessing(sample):\n",
    "    visible = tf.io.read_file(sample[0])\n",
    "    thermal = tf.io.read_file(sample[1])\n",
    "    visible = tf.io.decode_jpeg(visible)\n",
    "    thermal = tf.io.decode_jpeg(thermal)\n",
    "    visible = tf.cast(visible, tf.float32) / 127.5 - 1.\n",
    "    thermal = tf.cast(thermal, tf.float32) / 127.5 - 1.\n",
    "    return visible, thermal\n",
    "\n",
    "def create_dataset(data, mode, batch_size=1):\n",
    "    if mode == 'training':\n",
    "        return data.map(\n",
    "            data_train_preprocessing, \n",
    "            num_parallel_calls=AUTOTUNE, \n",
    "            deterministic=False).batch(\n",
    "            batch_size, \n",
    "            num_parallel_calls=AUTOTUNE, \n",
    "            deterministic=False, drop_remainder=True).cache().shuffle(len(data)).map(\n",
    "            data_train_augmentation, \n",
    "            num_parallel_calls=AUTOTUNE, \n",
    "            deterministic=False).prefetch(tf.data.AUTOTUNE)\n",
    "    elif mode == 'validation':\n",
    "        return data.map(\n",
    "            data_val_preprocessing, \n",
    "            num_parallel_calls=AUTOTUNE).batch(\n",
    "            BATCH_SIZE, \n",
    "            num_parallel_calls=AUTOTUNE, \n",
    "            drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        raise Exception(\"Invalid value for argument 'mode': %s. Supposed to be either 'training' or 'validation'.\"%(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0e1d8-c14c-44fc-8cea-3f90fc618786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history['gen_v2t_loss'], label='Generator V2T Loss')\n",
    "    plt.plot(history['gen_t2v_loss'], label='Generator T2V Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_model(gen_v2t, gen_t2v, data_val):\n",
    "    NUM = 9\n",
    "    plt.figure(figsize=(NUM+1, 5), dpi=300)\n",
    "    for i, (real_v, real_t) in enumerate(data_val.take(NUM)):\n",
    "        fake_t = gen_v2t(real_v, training=False)\n",
    "        fake_v = gen_t2v(real_t, training=False)\n",
    "        plt.subplot(4, NUM, i+1)\n",
    "        plt.imshow((tf.squeeze(real_v) + 1.)/2.)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4, NUM, i+NUM+1)\n",
    "        plt.imshow((tf.squeeze(fake_v) + 1.)/2.)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4, NUM, i+2*NUM+1)\n",
    "        plt.imshow((tf.squeeze(real_t) + 1.)/2., cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4, NUM, i+3*NUM+1)\n",
    "        plt.imshow((tf.squeeze(fake_t) + 1.)/2., cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.subplots_adjust(\n",
    "        top=1-0.5/5, \n",
    "        bottom=0.5/5, \n",
    "        left=0.5/(NUM+1), \n",
    "        right=1-0.5/(NUM+1), \n",
    "        wspace=0, \n",
    "        hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d5d76-423e-4a9a-b3fd-3e6d56e55633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN:\n",
    "    def __init__(self, disc_v, disc_t, gen_v2t, gen_t2v, disc_v_opt, disc_t_opt, gen_v2t_opt, gen_t2v_opt, pool_size=50):\n",
    "        self.disc_v = disc_v\n",
    "        self.disc_t = disc_t\n",
    "        self.gen_v2t = gen_v2t\n",
    "        self.gen_t2v = gen_t2v\n",
    "        self.disc_v_opt = disc_v_opt\n",
    "        self.disc_t_opt = disc_t_opt\n",
    "        self.gen_v2t_opt = gen_v2t_opt\n",
    "        self.gen_t2v_opt = gen_t2v_opt\n",
    "        self.pool_size = pool_size\n",
    "        self.pool_count = 0\n",
    "        self.pool_v = []\n",
    "        self.pool_t = []\n",
    "        self.MSE = MeanSquaredError()\n",
    "        self.MAE = MeanAbsoluteError()\n",
    "        self.history = {'disc_v_loss':[], 'disc_t_loss':[], 'gen_v2t_loss':[], 'gen_t2v_loss':[]}\n",
    "        self.disc_v_mean_loss = Mean(name='disc_v_loss')\n",
    "        self.disc_t_mean_loss = Mean(name='disc_t_loss')\n",
    "        self.gen_v2t_mean_loss = Mean(name='gen_v2t_loss')\n",
    "        self.gen_t2v_mean_loss = Mean(name='gen_t2v_loss')\n",
    "    \n",
    "    def update_fake_pool(self, image, pool):\n",
    "        selected = image\n",
    "        if self.pool_count < 50:\n",
    "            pool.append(image)\n",
    "        elif random.random() < 0.5:\n",
    "            index = random.randint(0, 49)\n",
    "            selected = pool[index]\n",
    "            pool[index] = image\n",
    "        return selected\n",
    "    \n",
    "    def update_discriminator(self, disc, gen, origin, real, pool, disc_opt, disc_mean_loss):\n",
    "        fake = gen(origin, training=False)\n",
    "        fake = self.update_fake_pool(fake, pool)\n",
    "        zeros_patch = tf.zeros([1,16,16,1])\n",
    "        ones_patch = tf.ones([1,16,16,1])\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            fake_pred = disc(fake)\n",
    "            real_pred = disc(real)\n",
    "            disc_loss = (self.MSE(zeros_patch, fake_pred) + self.MSE(ones_patch, real_pred)) / 2.\n",
    "        grad_of_disc = disc_tape.gradient(disc_loss, disc.trainable_variables)\n",
    "        disc_opt.apply_gradients(zip(grad_of_disc, disc.trainable_variables))\n",
    "        disc_mean_loss.update_state(disc_loss)\n",
    "    \n",
    "    def update_generator(self, disc, gen_o2t, gen_t2o, origin, target, gen_opt, gen_mean_loss):\n",
    "        ones_patch = tf.ones([1,16,16,1])\n",
    "        backward_fake = gen_t2o(target, training=False)\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            forward_fake = gen_o2t(origin)\n",
    "            forward_fake_pred = disc(forward_fake, training=False)\n",
    "            adversarial_loss = self.MSE(ones_patch, forward_fake_pred)\n",
    "            forward_cycle = gen_t2o(forward_fake, training=False)\n",
    "            forward_loss = self.MAE(origin, forward_cycle)\n",
    "            backward_cycle = gen_o2t(backward_fake)\n",
    "            backward_loss = self.MAE(target, backward_cycle)\n",
    "            gen_loss = adversarial_loss + 10*(forward_loss + backward_loss)\n",
    "        grad_of_gen = gen_tape.gradient(gen_loss, gen_o2t.trainable_variables)\n",
    "        gen_opt.apply_gradients(zip(grad_of_gen, gen_o2t.trainable_variables))\n",
    "        gen_mean_loss.update_state(gen_loss)\n",
    "    \n",
    "    @tf.function(jit_compile=True)\n",
    "    def train_step(self, real_v, real_t):\n",
    "        self.update_discriminator(self.disc_v, self.gen_t2v, real_t, real_v, self.pool_v, self.disc_v_opt, self.disc_v_mean_loss)\n",
    "        self.update_generator(self.disc_v, self.gen_t2v, self.gen_v2t, real_t, real_v, self.gen_t2v_opt, self.gen_t2v_mean_loss)\n",
    "        self.update_discriminator(self.disc_t, self.gen_v2t, real_v, real_t, self.pool_t, self.disc_t_opt, self.disc_t_mean_loss)\n",
    "        self.update_generator(self.disc_t, self.gen_v2t, self.gen_t2v, real_v, real_t, self.gen_v2t_opt, self.gen_v2t_mean_loss)\n",
    "        \n",
    "    def fit(self, visible_train, thermal_train, data_val, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            tic = perf_counter()\n",
    "            self.disc_v_mean_loss.reset_states()\n",
    "            self.disc_t_mean_loss.reset_states()\n",
    "            self.gen_v2t_mean_loss.reset_states()\n",
    "            self.gen_t2v_mean_loss.reset_states()\n",
    "            for real_v, real_t in tqdm(zip(visible_train, thermal_train), total=len(visible_train)):\n",
    "                model.train_step(real_v, real_t)\n",
    "                self.pool_count += 1\n",
    "            self.history['disc_v_loss'].append(self.disc_v_mean_loss.result().numpy())\n",
    "            self.history['disc_t_loss'].append(self.disc_t_mean_loss.result().numpy())\n",
    "            self.history['gen_v2t_loss'].append(self.gen_v2t_mean_loss.result().numpy())\n",
    "            self.history['gen_t2v_loss'].append(self.gen_t2v_mean_loss.result().numpy())\n",
    "            display.clear_output(wait=True)\n",
    "            print(\"Epoch %d/%d - %.1fs | disc_v_loss: %.5f - disc_t_loss: %.5f - gen_v2t_loss: %.5f - gen_t2v_loss: %.5f\"%(\n",
    "                epoch+1, epochs, perf_counter() - tic, \n",
    "                self.history['disc_v_loss'][-1], self.history['disc_t_loss'][-1], \n",
    "                self.history['gen_v2t_loss'][-1], self.history['gen_t2v_loss'][-1]))\n",
    "            plot_history(self.history)\n",
    "            test_model(self.gen_v2t, self.gen_t2v, data_val)\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9870a-7e7e-4474-ba8a-8ea30206649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./processed_data\"\n",
    "training_path = os.path.join(data_path, \"training\")\n",
    "validation_path = os.path.join(data_path, \"validation_2\")\n",
    "visible_training_path = os.path.join(training_path, \"visible\")\n",
    "thermal_training_path = os.path.join(training_path, \"thermal\")\n",
    "visible_validation_path = os.path.join(validation_path, \"visible\")\n",
    "thermal_validation_path = os.path.join(validation_path, \"thermal\")\n",
    "visible_training_files = [os.path.join(visible_training_path, filename) for filename in os.listdir(visible_training_path)]\n",
    "thermal_training_files = [os.path.join(thermal_training_path, filename) for filename in os.listdir(thermal_training_path)]\n",
    "visible_validation_files = [os.path.join(visible_validation_path, filename) for filename in sorted(os.listdir(visible_validation_path))]\n",
    "thermal_validation_files = [os.path.join(thermal_validation_path, filename) for filename in sorted(os.listdir(thermal_validation_path))]\n",
    "validation_files = list(zip(visible_validation_files, thermal_validation_files))\n",
    "random.shuffle(visible_training_files)\n",
    "random.shuffle(thermal_training_files)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 1\n",
    "visible_train = tf.data.Dataset.from_tensor_slices(visible_training_files)\n",
    "thermal_train = tf.data.Dataset.from_tensor_slices(thermal_training_files)\n",
    "data_val = tf.data.Dataset.from_tensor_slices(validation_files)\n",
    "visible_train = create_dataset(visible_train, 'training', batch_size=BATCH_SIZE)\n",
    "thermal_train = create_dataset(thermal_train, 'training', batch_size=BATCH_SIZE)\n",
    "data_val = create_dataset(data_val, 'validation', batch_size=BATCH_SIZE)\n",
    "print(\"Number of training data (pairs):\", len(visible_train))\n",
    "print(\"Number of validation data (pairs):\", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10e4d5-4173-4220-af1c-1607f04a8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_v = define_discriminator(input_shape=(256,256,3), name='Discriminator_V')\n",
    "disc_t = define_discriminator(input_shape=(256,256,1), name='Discriminator_T')\n",
    "gen_v2t = define_generator((256,256,3), 1, n_resnet=9, name='Generator_V2T')\n",
    "gen_t2v = define_generator((256,256,1), 3, n_resnet=9, name='Generator_T2V')\n",
    "disc_v_opt = Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "disc_t_opt = Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "gen_v2t_opt = Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "gen_t2v_opt = Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "POOL_SIZE = 50\n",
    "model = CycleGAN(disc_v, disc_t, gen_v2t, gen_t2v, disc_v_opt, disc_t_opt, gen_v2t_opt, gen_t2v_opt, pool_size=POOL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d66406-49e3-4651-80ac-0c0a3e8d87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.utils.plot_model(disc_v, to_file='Discriminator_V.png', show_shapes=True)\n",
    "#keras.utils.plot_model(disc_t, to_file='Discriminator_T.png', show_shapes=True)\n",
    "#keras.utils.plot_model(gen_v2t, to_file='Generator_V2T.png', show_shapes=True)\n",
    "#keras.utils.plot_model(gen_t2v, to_file='Generator_T2V.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29201fc-c3f5-4d9a-b55b-d0a9ad189f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 25\n",
    "history = model.fit(visible_train, thermal_train, data_val, epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee20e9-043e-4352-b70d-444be414311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gen_v2t.save(\"./cyclegan/v2t\", include_optimizer=False)\n",
    "model.gen_t2v.save(\"./cyclegan/t2v\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f494ebf-c151-4011-ac28-ecd9a62166d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
