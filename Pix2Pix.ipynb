{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ae7da-7eec-4b8d-b338-b43d0e76da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Concatenate, Dropout, LeakyReLU, Activation, Layer, InputSpec\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanAbsoluteError\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from IPython import display\n",
    "from time import perf_counter\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# preserve threads for GPU\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "\n",
    "# constrain VRAM usage\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# enable mixed-precision training\n",
    "keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e0709-e4a0-4cf4-abf3-0284489385f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(sample):\n",
    "    visible = tf.io.read_file(sample[0])\n",
    "    thermal = tf.io.read_file(sample[1])\n",
    "    visible = tf.io.decode_jpeg(visible)\n",
    "    thermal = tf.io.decode_jpeg(thermal)\n",
    "    visible = tf.cast(visible, tf.float32) / 127.5 - 1.\n",
    "    thermal = tf.cast(thermal, tf.float32) / 127.5 - 1.\n",
    "    return visible, thermal\n",
    "\n",
    "def data_augmentation(visible, thermal):\n",
    "    if random.random() < 0.5:\n",
    "        visible = tf.image.flip_left_right(visible)\n",
    "        thermal = tf.image.flip_left_right(thermal)\n",
    "    if random.random() < 0.5:\n",
    "        visible = tf.image.flip_up_down(visible)\n",
    "        thermal = tf.image.flip_up_down(thermal)\n",
    "    if random.random() < 0.5:\n",
    "        visible = tf.image.rot90(visible)\n",
    "        thermal = tf.image.rot90(thermal)\n",
    "    return visible, thermal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f71b27-6717-4656-aa08-671ff9a96f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(visible_shape=(256,256,3), thermal_shape=(256,256,1)):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    visible_image = Input(shape=visible_shape)\n",
    "    thermal_image = Input(shape=thermal_shape)\n",
    "    merged = Concatenate()([visible_image, thermal_image])\n",
    "    d = Conv2D(64, 4, strides=2, padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(128, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(256, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(512, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(512, 4, padding='same', kernel_initializer=init, use_bias=False)(d)\n",
    "    d = InstanceNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    output = Conv2D(1, 4, padding='same', kernel_initializer=init, name='Patch', dtype=tf.float32)(d)\n",
    "    model = Model([visible_image, thermal_image], output, name='Discriminator')\n",
    "    return model\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding, **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = (input_shape[0], input_shape[1] + 2*self.padding[0], input_shape[2] + 2*self.padding[1], input_shape[3])\n",
    "        return shape\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        width_pad, height_pad = self.padding\n",
    "        return tf.pad(x, [[0, 0], [height_pad, height_pad], [width_pad, width_pad], [0, 0]], 'REFLECT')\n",
    "\n",
    "def resnet_block(n_filters, input_layer):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    g = ReflectionPadding2D(padding=(1, 1))(input_layer)\n",
    "    g = Conv2D(n_filters, 3, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = ReflectionPadding2D(padding=(1, 1))(g)\n",
    "    g = Conv2D(n_filters, 3, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Concatenate()([g, input_layer])\n",
    "    return g\n",
    "\n",
    "def define_generator(input_shape=(256,256,3), n_resnet=9):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    input_image = Input(shape=input_shape)\n",
    "    g = ReflectionPadding2D(padding=(3, 3))(input_image)\n",
    "    g = Conv2D(64, 7, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = ReflectionPadding2D(padding=(1, 1))(g)\n",
    "    g = Conv2D(128, 3, strides=2, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = ReflectionPadding2D(padding=(1, 1))(g)\n",
    "    g = Conv2D(256, 3, strides=2, kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    for _ in range(n_resnet):\n",
    "        g = resnet_block(256, g)\n",
    "    g = Conv2DTranspose(128, 3, strides=2, padding='same', kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Conv2DTranspose(64, 3, strides=2, padding='same', kernel_initializer=init, use_bias=False)(g)\n",
    "    g = InstanceNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = ReflectionPadding2D(padding=(3, 3))(g)\n",
    "    g = Conv2D(1, 7, kernel_initializer=init)(g)\n",
    "    output_image = Activation('tanh', dtype=tf.float32)(g)\n",
    "    model = Model(input_image, output_image)\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "def encoder_block(input_layer, n_filters, layernorm=True):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    if layernorm:\n",
    "        g = Conv2D(n_filters, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(input_layer)\n",
    "        g = InstanceNormalization()(g)\n",
    "    else:\n",
    "        g = Conv2D(n_filters, 4, strides=2, padding='same', kernel_initializer=init)(input_layer)\n",
    "    g_a = LeakyReLU(alpha=0.2)(g)\n",
    "    return g, g_a\n",
    "\n",
    "def decoder_block(input_layer, skip_in, n_filters, dropout=True):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    g = Conv2DTranspose(n_filters, 4, strides=2, padding='same', kernel_initializer=init, use_bias=False)(input_layer)\n",
    "    g = InstanceNormalization()(g)\n",
    "    if dropout:\n",
    "        g = Dropout(0.5)(g)\n",
    "    g = Concatenate()([g, skip_in])\n",
    "    g = Activation('relu')(g)\n",
    "    return g\n",
    "\n",
    "def define_generator(visible_shape=(256,256,3)):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    visible_image = Input(shape=visible_shape)\n",
    "    e1, e1_a = encoder_block(visible_image, 64, layernorm=False)\n",
    "    e2, e2_a = encoder_block(e1_a, 128)\n",
    "    e3, e3_a = encoder_block(e2_a, 256)\n",
    "    e4, e4_a = encoder_block(e3_a, 512)\n",
    "    e5, e5_a = encoder_block(e4_a, 512)\n",
    "    e6, e6_a = encoder_block(e5_a, 512)\n",
    "    e7, e7_a = encoder_block(e6_a, 512)\n",
    "    v = Conv2D(512, 4, strides=2, padding='same', activation='relu', kernel_initializer=init)(e7_a)\n",
    "    d1 = decoder_block(v, e7, 512)\n",
    "    d2 = decoder_block(d1, e6, 512)\n",
    "    d3 = decoder_block(d2, e5, 512)\n",
    "    d4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "    d5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "    d6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "    d7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "    thermal_image = Conv2DTranspose(1, 4, strides=2, padding='same', activation='tanh', dtype=tf.float32, kernel_initializer=init)(d7)\n",
    "    model = Model(visible_image, thermal_image, name='Generator')\n",
    "    return model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fcd6e3-d21c-4aab-abcc-049904e45577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    #plt.plot(history['disc_loss'], label='Discriminator Loss')\n",
    "    plt.plot(history['gen_loss'], label='Generator Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_model(gen, data_val):\n",
    "    NUM = 9\n",
    "    plt.figure(figsize=(NUM+1, 4), dpi=300)\n",
    "    for i, (visible, thermal) in enumerate(data_val.take(NUM)):\n",
    "        fake = gen(visible, training=False)\n",
    "        plt.subplot(3, NUM, i+1)\n",
    "        plt.imshow((tf.squeeze(visible) + 1.)/2.)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(3, NUM, i+NUM+1)\n",
    "        plt.imshow((tf.squeeze(fake) + 1.)/2., cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(3, NUM, i+2*NUM+1)\n",
    "        plt.imshow((tf.squeeze(thermal) + 1.)/2., cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.subplots_adjust(\n",
    "        top=1-0.5/4, \n",
    "        bottom=0.5/4, \n",
    "        left=0.5/(NUM+1), \n",
    "        right=1-0.5/(NUM+1), \n",
    "        wspace=0, \n",
    "        hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85bac6-0336-4a2a-b6a3-804c500860ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2Pix:\n",
    "    def __init__(self, disc, gen, disc_opt, gen_opt):\n",
    "        self.disc = disc\n",
    "        self.gen = gen\n",
    "        self.disc_opt = disc_opt\n",
    "        self.gen_opt = gen_opt\n",
    "        self.BCE = BinaryCrossentropy(from_logits=True)\n",
    "        self.MAE = MeanAbsoluteError()\n",
    "        self.history = {'disc_loss':[], 'gen_loss':[]}\n",
    "        self.disc_mean_loss = Mean(name='disc_loss')\n",
    "        self.gen_mean_loss = Mean(name='gen_loss')\n",
    "    \n",
    "    @tf.function(jit_compile=True)\n",
    "    def train_step(self, visible, thermal):\n",
    "        fake = self.gen(visible, training=False)\n",
    "        zeros_patch = tf.zeros([1,16,16,1])\n",
    "        ones_patch = tf.ones([1,16,16,1])\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            fake_pred = self.disc([visible, fake])\n",
    "            thermal_pred = self.disc([visible, thermal])\n",
    "            disc_loss = (self.BCE(zeros_patch, fake_pred) + self.BCE(ones_patch, thermal_pred)) / 2.\n",
    "        grad_of_disc = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
    "        self.disc_opt.apply_gradients(zip(grad_of_disc, self.disc.trainable_variables))\n",
    "        self.disc_mean_loss.update_state(disc_loss)\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake = self.gen(visible)\n",
    "            fake_pred = self.disc([visible, fake], training=False)\n",
    "            bce_loss = self.BCE(ones_patch, fake_pred)\n",
    "            mae_loss = self.MAE(thermal, fake)\n",
    "            gen_loss = bce_loss + 100*mae_loss\n",
    "        grad_of_gen = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
    "        self.gen_opt.apply_gradients(zip(grad_of_gen, self.gen.trainable_variables))\n",
    "        self.gen_mean_loss.update_state(mae_loss)\n",
    "    \n",
    "    def fit(self, data_train, data_val, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            tic = perf_counter()\n",
    "            self.disc_mean_loss.reset_states()\n",
    "            self.gen_mean_loss.reset_states()\n",
    "            for visible, thermal in tqdm(data_train):\n",
    "                model.train_step(visible, thermal)\n",
    "            self.history['disc_loss'].append(self.disc_mean_loss.result().numpy())\n",
    "            self.history['gen_loss'].append(self.gen_mean_loss.result().numpy())\n",
    "            display.clear_output(wait=True)\n",
    "            print(\"Epoch %d/%d - %.1fs | disc_loss: %.5f - gen_loss: %.5f\"%(\n",
    "                epoch+1, epochs, perf_counter() - tic, self.history['disc_loss'][-1], self.history['gen_loss'][-1]))\n",
    "            plot_history(self.history)\n",
    "            test_model(self.gen, data_val)\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abba418-b0ee-4096-b647-c2e0f61921ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./processed_data\"\n",
    "training_path = os.path.join(data_path, \"training\")\n",
    "validation_path = os.path.join(data_path, \"validation\")\n",
    "visible_training_path = os.path.join(training_path, \"visible\")\n",
    "thermal_training_path = os.path.join(training_path, \"thermal\")\n",
    "visible_validation_path = os.path.join(validation_path, \"visible\")\n",
    "thermal_validation_path = os.path.join(validation_path, \"thermal\")\n",
    "visible_training_files = [os.path.join(visible_training_path, filename) for filename in sorted(os.listdir(visible_training_path))]\n",
    "thermal_training_files = [os.path.join(thermal_training_path, filename) for filename in sorted(os.listdir(thermal_training_path))]\n",
    "visible_validation_files = [os.path.join(visible_validation_path, filename) for filename in sorted(os.listdir(visible_validation_path))]\n",
    "thermal_validation_files = [os.path.join(thermal_validation_path, filename) for filename in sorted(os.listdir(thermal_validation_path))]\n",
    "training_files = list(zip(visible_training_files, thermal_training_files))\n",
    "validation_files = list(zip(visible_validation_files, thermal_validation_files))\n",
    "random.shuffle(training_files)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 1\n",
    "data_train = tf.data.Dataset.from_tensor_slices(training_files)\n",
    "data_val = tf.data.Dataset.from_tensor_slices(validation_files)\n",
    "data_train = data_train.map(\n",
    "    data_preprocessing, \n",
    "    num_parallel_calls=AUTOTUNE, \n",
    "    deterministic=False).batch(\n",
    "    BATCH_SIZE, \n",
    "    num_parallel_calls=AUTOTUNE, \n",
    "    deterministic=False, drop_remainder=True).cache().shuffle(len(data_train)).map(\n",
    "    data_augmentation, \n",
    "    num_parallel_calls=AUTOTUNE, \n",
    "    deterministic=False).prefetch(tf.data.AUTOTUNE)\n",
    "data_val = data_val.map(\n",
    "    data_preprocessing, \n",
    "    num_parallel_calls=AUTOTUNE).batch(\n",
    "    BATCH_SIZE, \n",
    "    num_parallel_calls=AUTOTUNE, \n",
    "    drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\n",
    "print(\"Number of training data (pairs):\", len(data_train))\n",
    "print(\"Number of validation data (pairs):\", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee3bd5-47ac-4e7d-9573-a5d9daf0b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = define_discriminator()\n",
    "gen = define_generator()\n",
    "disc_opt = Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "gen_opt = Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "model = Pix2Pix(disc, gen, disc_opt, gen_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2280de4-d342-4945-a24e-5a2aa8fcc027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.utils.plot_model(disc, to_file='Discriminator.png', show_shapes=True)\n",
    "#keras.utils.plot_model(gen, to_file='Generator.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0a027-5060-4bb5-9815-ba7f7b32213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 75\n",
    "history = model.fit(data_train, data_val, epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99be078-9562-4f15-9366-7ebb9b750c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gen.save(\"./pix2pix\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f896941-6574-4419-a577-c3db6b9ce82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_pix2pix = keras.models.load_model(\"./pix2pix\", compile=False)\n",
    "gen_cyclegan = keras.models.load_model(\"./cyclegan/v2t\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fd251-38da-4201-8ea2-f40de3b18dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model.gen, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a4c576-01ef-4c26-a6ee-44082f384b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL = 9\n",
    "ROW = 4\n",
    "plt.figure(figsize=(COL+1, ROW+1), dpi=300)\n",
    "for i, (real_v, real_t) in enumerate(data_val.take(COL)):\n",
    "    fake_t_pix2pix = gen_pix2pix(real_v, training=False)\n",
    "    fake_t_cyclegan = gen_cyclegan(real_v, training=False)\n",
    "    plt.subplot(ROW, COL, i+1)\n",
    "    plt.imshow((tf.squeeze(real_v) + 1.)/2.)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(ROW, COL, i+COL+1)\n",
    "    plt.imshow((tf.squeeze(fake_t_pix2pix) + 1.)/2., cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(ROW, COL, i+2*COL+1)\n",
    "    plt.imshow((tf.squeeze(fake_t_cyclegan) + 1.)/2., cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(ROW, COL, i+3*COL+1)\n",
    "    plt.imshow((tf.squeeze(real_t) + 1.)/2., cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(\n",
    "    top=1-0.5/(ROW+1), \n",
    "    bottom=0.5/(ROW+1), \n",
    "    left=0.5/(COL+1), \n",
    "    right=1-0.5/(COL+1), \n",
    "    wspace=0, \n",
    "    hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e70ef5-cdcd-4c4b-9e60-fa807dd0b14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
